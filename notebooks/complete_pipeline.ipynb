{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv11 Poo Detection Pipeline - Complete Implementation\n",
    "\n",
    "This notebook implements the complete pipeline for training a YOLOv11 poo detection model optimized for mobile deployment using LiteRT.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Project Setup**: Clone repository and install dependencies\n",
    "2. **Dataset Download**: Download and extract semantic_masks dataset\n",
    "3. **Data Preprocessing**: Convert semantic masks to YOLO format\n",
    "4. **Model Training**: Train YOLOv11 model with customizable parameters\n",
    "5. **Model Evaluation**: Evaluate performance and visualize results\n",
    "6. **LiteRT Export**: Export optimized model for Android deployment\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sciber/deepoo-yolo11-litert/blob/main/notebooks/complete_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Pipeline Configuration - Customize as needed\n",
    "CONFIG = {\n",
    "    # Project settings\n",
    "    'project_name': 'deepoo-yolo11-litert',\n",
    "    'project_url': 'https://github.com/your-username/deepoo-yolo11-litert.git',\n",
    "    'base_dir': '/content' if 'google.colab' in sys.modules else '.',\n",
    "    \n",
    "    # Training settings\n",
    "    'model_variant': 'yolo11n.pt',  # yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt\n",
    "    'epochs': 100,\n",
    "    'imgsz': 640,\n",
    "    'batch_size': -1,  # Auto batch size\n",
    "    'patience': 50,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'visualize_samples': 10,\n",
    "    'conf_threshold': 0.25,\n",
    "    'iou_threshold': 0.45,\n",
    "    \n",
    "    # Export settings\n",
    "    'quantize': True,\n",
    "    'optimize': True,\n",
    "    'simplify': True,\n",
    "}\n",
    "\n",
    "print(\"Pipeline Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project directory\n",
    "project_dir = Path(CONFIG['base_dir']) / CONFIG['project_name']\n",
    "\n",
    "# Clone repository if not exists\n",
    "if not project_dir.exists():\n",
    "    print(\"Cloning repository...\")\n",
    "    subprocess.run(['git', 'clone', CONFIG['project_url'], str(project_dir)], check=True)\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists.\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "data_dir = Path('data')\n",
    "dataset_path = data_dir / 'semantic_masks'\n",
    "\n",
    "# Create dataset structure\n",
    "(dataset_path / 'images').mkdir(parents=True, exist_ok=True)\n",
    "(dataset_path / 'masks').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset structure created at: {dataset_path}\")\n",
    "print(\"\\nNote: Please place your semantic_masks dataset in the data/semantic_masks directory\")\n",
    "print(\"Expected structure:\")\n",
    "print(\"  data/semantic_masks/\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ images/  # Original images from cameras A & B\")\n",
    "print(\"  ‚îî‚îÄ‚îÄ masks/   # Corresponding bitmap masks\")\n",
    "\n",
    "# Verify dataset\n",
    "images_dir = dataset_path / 'images'\n",
    "masks_dir = dataset_path / 'masks'\n",
    "image_count = len(list(images_dir.glob('*')))\n",
    "mask_count = len(list(masks_dir.glob('*')))\n",
    "print(f\"\\nDataset verification: {image_count} images, {mask_count} masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "print(\"Starting data preprocessing...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, 'src/data/preprocess.py'], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    \n",
    "    print(\"‚úÖ Preprocessing completed successfully!\")\n",
    "    print(f\"Duration: {datetime.now() - start_time}\")\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(\"\\nOutput:\", result.stdout)\n",
    "        \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Preprocessing failed: {e}\")\n",
    "    if e.stderr:\n",
    "        print(f\"Error: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "\n",
    "train_cmd = [\n",
    "    sys.executable, 'src/models/train.py',\n",
    "    '--data', 'data/boxed_640x640/dataset.yaml',\n",
    "    '--output', 'models/',\n",
    "    '--model', CONFIG['model_variant'],\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--imgsz', str(CONFIG['imgsz']),\n",
    "    '--batch', str(CONFIG['batch_size']),\n",
    "    '--patience', str(CONFIG['patience'])\n",
    "]\n",
    "\n",
    "print(f\"Training command: {' '.join(train_cmd)}\")\n",
    "print(f\"\\nStarting training ({CONFIG['epochs']} epochs)...\")\n",
    "\n",
    "training_start = datetime.now()\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(train_cmd, capture_output=True, text=True, check=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "    print(f\"Duration: {datetime.now() - training_start}\")\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(\"\\nTraining output:\", result.stdout[-1000:])  # Last 1000 chars\n",
    "        \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    if e.stderr:\n",
    "        print(f\"Error: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model exists\n",
    "model_path = Path('models/best.pt')\n",
    "if not model_path.exists():\n",
    "    print(f\"‚ùå Model not found at {model_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model found: {model_path} ({model_path.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    eval_cmd = [\n",
    "        sys.executable, 'src/models/eval.py',\n",
    "        '--model', str(model_path),\n",
    "        '--data', 'data/boxed_640x640/dataset.yaml',\n",
    "        '--visualize', str(CONFIG['visualize_samples']),\n",
    "        '--conf', str(CONFIG['conf_threshold']),\n",
    "        '--iou', str(CONFIG['iou_threshold'])\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(eval_cmd, capture_output=True, text=True, check=True)\n",
    "        \n",
    "        print(\"‚úÖ Evaluation completed successfully!\")\n",
    "        if result.stdout:\n",
    "            print(\"\\nEvaluation results:\", result.stdout)\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LiteRT Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to LiteRT\n",
    "if model_path.exists():\n",
    "    export_cmd = [\n",
    "        sys.executable, 'src/models/export_litert.py',\n",
    "        '--model', str(model_path),\n",
    "        '--output', 'models/',\n",
    "        '--imgsz', str(CONFIG['imgsz'])\n",
    "    ]\n",
    "    \n",
    "    if CONFIG['quantize']:\n",
    "        export_cmd.extend(['--quantize', '--data', 'data/boxed_640x640/dataset.yaml'])\n",
    "    if CONFIG['optimize']:\n",
    "        export_cmd.append('--optimize')\n",
    "    if CONFIG['simplify']:\n",
    "        export_cmd.append('--simplify')\n",
    "    \n",
    "    print(f\"Export command: {' '.join(export_cmd)}\")\n",
    "    print(\"\\nStarting LiteRT export...\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(export_cmd, capture_output=True, text=True, check=True)\n",
    "        \n",
    "        print(\"‚úÖ Export completed successfully!\")\n",
    "        if result.stdout:\n",
    "            print(\"\\nExport results:\", result.stdout)\n",
    "            \n",
    "        # Check for exported model\n",
    "        tflite_files = list(Path('models').glob('*.tflite'))\n",
    "        if tflite_files:\n",
    "            for tflite_file in tflite_files:\n",
    "                size_mb = tflite_file.stat().st_size / (1024*1024)\n",
    "                print(f\"\\nüì± Exported model: {tflite_file} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Export failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot export - model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline completion summary\n",
    "print(\"üéâ Pipeline Execution Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check preprocessing\n",
    "processed_dir = Path('data/boxed_640x640')\n",
    "if processed_dir.exists():\n",
    "    print(\"‚úÖ Data preprocessing: COMPLETED\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        count = len(list((processed_dir / 'images' / split).glob('*.jpg')))\n",
    "        print(f\"   {split}: {count} images\")\n",
    "else:\n",
    "    print(\"‚ùå Data preprocessing: FAILED\")\n",
    "\n",
    "# Check training\n",
    "if model_path.exists():\n",
    "    print(f\"‚úÖ Model training: COMPLETED ({model_path.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå Model training: FAILED\")\n",
    "\n",
    "# Check evaluation\n",
    "eval_dir = Path('data/evaluation/predictions')\n",
    "if eval_dir.exists():\n",
    "    vis_count = len(list(eval_dir.glob('**/*_pred.jpg')))\n",
    "    print(f\"‚úÖ Model evaluation: COMPLETED ({vis_count} visualizations)\")\n",
    "else:\n",
    "    print(\"‚ùå Model evaluation: FAILED\")\n",
    "\n",
    "# Check export\n",
    "tflite_files = list(Path('models').glob('*.tflite'))\n",
    "if tflite_files:\n",
    "    total_size = sum(f.stat().st_size for f in tflite_files) / (1024*1024)\n",
    "    print(f\"‚úÖ LiteRT export: COMPLETED ({len(tflite_files)} models, {total_size:.2f} MB total)\")\n",
    "    for f in tflite_files:\n",
    "        print(f\"   üì± {f.name} ({f.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå LiteRT export: FAILED\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for Android integration!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Copy .tflite file to Android app assets\")\n",
    "print(\"2. Use TensorFlow Lite Android API\")\n",
    "print(\"3. Input: RGB images 640x640, normalized [0,1]\")\n",
    "print(\"4. Output: YOLO detection format\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
